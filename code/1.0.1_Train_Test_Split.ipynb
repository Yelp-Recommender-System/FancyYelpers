{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aBWKb49Xfkv1946YN7_SIg</td>\n",
       "      <td>sSPbLBHcEMXaJfoO8zs1bA</td>\n",
       "      <td>poSV39UqEg-gpESXafS9-g</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-01-17 05:33:14</td>\n",
       "      <td>Angry Crab Shack</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Restaurants, American (New), Seafood, Cajun/Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>jCHaWXgppmZjkOdpFltWGA</td>\n",
       "      <td>D5ywfFmwtJxLReqAYlDDmw</td>\n",
       "      <td>poSV39UqEg-gpESXafS9-g</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-01-30 01:13:29</td>\n",
       "      <td>Angry Crab Shack</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Restaurants, American (New), Seafood, Cajun/Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>KvJ8yT-dODmCYe21J-Pp_A</td>\n",
       "      <td>gjoN4q-f61kwjmmU4mm1_g</td>\n",
       "      <td>poSV39UqEg-gpESXafS9-g</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-03-30 22:28:36</td>\n",
       "      <td>Angry Crab Shack</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Restaurants, American (New), Seafood, Cajun/Cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               review_id                 user_id  \\\n",
       "0           1  aBWKb49Xfkv1946YN7_SIg  sSPbLBHcEMXaJfoO8zs1bA   \n",
       "1           2  jCHaWXgppmZjkOdpFltWGA  D5ywfFmwtJxLReqAYlDDmw   \n",
       "2           3  KvJ8yT-dODmCYe21J-Pp_A  gjoN4q-f61kwjmmU4mm1_g   \n",
       "\n",
       "              business_id  stars                 date     business_name  \\\n",
       "0  poSV39UqEg-gpESXafS9-g    5.0  2016-01-17 05:33:14  Angry Crab Shack   \n",
       "1  poSV39UqEg-gpESXafS9-g    5.0  2016-01-30 01:13:29  Angry Crab Shack   \n",
       "2  poSV39UqEg-gpESXafS9-g    2.0  2016-03-30 22:28:36  Angry Crab Shack   \n",
       "\n",
       "      city state                                         categories  \n",
       "0  Phoenix    AZ  Restaurants, American (New), Seafood, Cajun/Cr...  \n",
       "1  Phoenix    AZ  Restaurants, American (New), Seafood, Cajun/Cr...  \n",
       "2  Phoenix    AZ  Restaurants, American (New), Seafood, Cajun/Cr...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this data is based on the output of 1.0.0_Data_Filtering\n",
    "data = pd.read_csv('../data/filtered_reviews_in_Phonex.csv')\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(data, m, n):\n",
    "\n",
    "    '''\n",
    "    construct rating matrix from data\n",
    "    the columns of which represent business_id\n",
    "    the rows of which represent user_id\n",
    "    the values of whose elements represent the according ratings\n",
    "    @ data: filterd_reviews \n",
    "    @ m: counts of ratings for validation\n",
    "    @ n: counts of ratings for test\n",
    "    '''\n",
    "\n",
    "    # to construct sparse matrix\n",
    "    # train\n",
    "    train_user_id = []\n",
    "    train_business_id = []\n",
    "    train_stars = []\n",
    "    # validation\n",
    "    valid_user_id = []\n",
    "    valid_business_id = []\n",
    "    valid_stars = []\n",
    "    # test\n",
    "    test_user_id = []\n",
    "    test_business_id = []\n",
    "    test_stars = []\n",
    "    \n",
    "    user_id_lst = data['user_id'].unique().tolist() # rows of sparse matrix\n",
    "    busi_id_lst = data['business_id'].unique().tolist() # columns of sparse matrix\n",
    "\n",
    "    train_sparse_matrix = np.zeros(shape=(len(user_id_lst), len(busi_id_lst)))\n",
    "    valid_sparse_matrix = np.zeros(shape=(len(user_id_lst), len(busi_id_lst)))\n",
    "    test_sparse_matrix = np.zeros(shape=(len(user_id_lst), len(busi_id_lst)))\n",
    "\n",
    "    ranking_df = data[['user_id','business_id','stars','date']].groupby(['user_id'])\n",
    "    \n",
    "    for group_name, group_df in ranking_df:\n",
    "        group_df.sort_values(by='date')\n",
    "\n",
    "        # if the len(group_df) > valid_m + test_n, split the group_df as \n",
    "        # training set : group_df.iloc[:len(group_df)-m-n, :]\n",
    "        # validation set : group_df.iloc[len(group_df)-m-n:len(group_df)-n, :]\n",
    "        # test set : group_df.iloc[len(group_df)-n:, :]\n",
    "\n",
    "        # otherwise, not split the group_df\n",
    "        # keep the group_df as training set\n",
    "\n",
    "        if len(group_df) > m+n: \n",
    "\n",
    "            training_set = group_df.iloc[:len(group_df)-m-n, :]\n",
    "            train_user_id.extend(training_set.loc[:,'user_id'].tolist()) \n",
    "            train_business_id.extend(training_set.loc[:,'business_id'].tolist())\n",
    "            train_stars.extend(training_set.loc[:,'stars'].tolist())\n",
    "\n",
    "            validation_set = group_df.iloc[len(group_df)-m-n:len(group_df)-n, :]\n",
    "            valid_user_id.extend(validation_set.loc[:,'user_id'].tolist()) \n",
    "            valid_business_id.extend(validation_set.loc[:,'business_id'].tolist())\n",
    "            valid_stars.extend(validation_set.loc[:,'stars'].tolist())\n",
    "\n",
    "            testing_set = group_df.iloc[len(group_df)-n:, :]\n",
    "            test_user_id.extend(testing_set.loc[:,'user_id'].tolist()) \n",
    "            test_business_id.extend(testing_set.loc[:,'business_id'].tolist())\n",
    "            test_stars.extend(testing_set.loc[:,'stars'].tolist())\n",
    "\n",
    "        else:\n",
    "            training_set = group_df\n",
    "            train_user_id.extend(training_set.loc[:,'user_id'].tolist()) \n",
    "            train_business_id.extend(training_set.loc[:,'business_id'].tolist())\n",
    "            train_stars.extend(training_set.loc[:,'stars'].tolist())\n",
    "\n",
    "    train_df = pd.DataFrame({'user_id': train_user_id, 'business_id': train_business_id, 'stars': train_stars})\n",
    "    valid_df = pd.DataFrame({'user_id': valid_user_id, 'business_id': valid_business_id, 'stars': valid_stars})\n",
    "    test_df = pd.DataFrame({'user_id': test_user_id, 'business_id': test_business_id, 'stars': test_stars})\n",
    "\n",
    "\n",
    "    for i in range(len(train_df)):\n",
    "        ratings = train_df.iloc[i, 2] # stars\n",
    "        row_index = user_id_lst.index(train_df.iloc[i, 0]) # user_id\n",
    "        column_index = busi_id_lst.index(train_df.iloc[i, 1]) # business_id\n",
    "        train_sparse_matrix[row_index, column_index] = ratings\n",
    "\n",
    "    for i in range(len(valid_df)):\n",
    "        ratings = valid_df.iloc[i, 2] # stars\n",
    "        row_index = user_id_lst.index(valid_df.iloc[i, 0]) # user_id\n",
    "        column_index = busi_id_lst.index(valid_df.iloc[i, 1]) # business_id\n",
    "        valid_sparse_matrix[row_index, column_index] = ratings\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        ratings = test_df.iloc[i, 2] # stars\n",
    "        row_index = user_id_lst.index(test_df.iloc[i, 0]) # user_id\n",
    "        column_index = busi_id_lst.index(test_df.iloc[i, 1]) # business_id\n",
    "        test_sparse_matrix[row_index, column_index] = ratings\n",
    "\n",
    "    # calculate sparstiy of the matrix\n",
    "    train_sparsity = 1 - np.count_nonzero(train_sparse_matrix)/ (train_sparse_matrix.shape[0] * train_sparse_matrix.shape[1])\n",
    "    valid_sparsity = 1 - np.count_nonzero(valid_sparse_matrix)/ (valid_sparse_matrix.shape[0] * valid_sparse_matrix.shape[1])\n",
    "    test_sparsity = 1 - np.count_nonzero(test_sparse_matrix)/ (test_sparse_matrix.shape[0] * test_sparse_matrix.shape[1])\n",
    "\n",
    "    train_sparsity *= 100\n",
    "    valid_sparsity *=100\n",
    "    test_sparsity *= 100\n",
    "\n",
    "    print (f'{len(user_id_lst)} users')\n",
    "    print (f'{len(busi_id_lst)} business')\n",
    "\n",
    "    print (f'Train_rating_matrix Sparsity: {round(train_sparsity,4)}%')\n",
    "    print (f'Valid_rating_matrix Sparsity: {round(valid_sparsity,4)}%')\n",
    "    print(f'Test_rating_matrix Sparsity:  {round(test_sparsity,4)}%')\n",
    "\n",
    "\n",
    "    return train_sparse_matrix, valid_sparse_matrix, test_sparse_matrix, train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20472 users\n",
      "1782 business\n",
      "Train_rating_matrix Sparsity: 99.4921%\n",
      "Valid_rating_matrix Sparsity: 99.9444%\n",
      "Test_rating_matrix Sparsity:  99.9444%\n"
     ]
    }
   ],
   "source": [
    "train_sparse_matrix, valid_sparse_matrix, test_sparse_matrix, train_df, valid_df, test_df = train_valid_test_split(data=data, m=1, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_sparse_matrix.npy', train_sparse_matrix)\n",
    "np.save('valid_sparse_matrix.npy', valid_sparse_matrix)\n",
    "np.save('test_sparse_matrix.npy', test_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--2HUmLkcNHZp0xw6AMBPg</td>\n",
       "      <td>zidkKI_N1OPxsiddTOQH_Q</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--2HUmLkcNHZp0xw6AMBPg</td>\n",
       "      <td>YOD9dXrnpu8HTRILpF0onw</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--2HUmLkcNHZp0xw6AMBPg</td>\n",
       "      <td>YOD9dXrnpu8HTRILpF0onw</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--2HUmLkcNHZp0xw6AMBPg</td>\n",
       "      <td>uTCOEqjuVAXUOzti5TWj2Q</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--2HUmLkcNHZp0xw6AMBPg</td>\n",
       "      <td>APXWKd1N-COyUdncd_FdyQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197092</th>\n",
       "      <td>zzO9aMo33jA3pPv8SoYskw</td>\n",
       "      <td>EhplcymNbSX5TvPgGilL7Q</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197093</th>\n",
       "      <td>zzO9aMo33jA3pPv8SoYskw</td>\n",
       "      <td>pDewiJY6KCcZgLxxgxg13Q</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197094</th>\n",
       "      <td>zzO9aMo33jA3pPv8SoYskw</td>\n",
       "      <td>ylxqmxh2gO1yCpQkIk6o3A</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197095</th>\n",
       "      <td>zzO9aMo33jA3pPv8SoYskw</td>\n",
       "      <td>5eK_pgro9_LxPYDoRVJnEA</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197096</th>\n",
       "      <td>zzO9aMo33jA3pPv8SoYskw</td>\n",
       "      <td>frCxZS7lPhEnQRJ3UY6m7A</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197097 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  stars\n",
       "0       --2HUmLkcNHZp0xw6AMBPg  zidkKI_N1OPxsiddTOQH_Q    5.0\n",
       "1       --2HUmLkcNHZp0xw6AMBPg  YOD9dXrnpu8HTRILpF0onw    5.0\n",
       "2       --2HUmLkcNHZp0xw6AMBPg  YOD9dXrnpu8HTRILpF0onw    5.0\n",
       "3       --2HUmLkcNHZp0xw6AMBPg  uTCOEqjuVAXUOzti5TWj2Q    5.0\n",
       "4       --2HUmLkcNHZp0xw6AMBPg  APXWKd1N-COyUdncd_FdyQ    5.0\n",
       "...                        ...                     ...    ...\n",
       "197092  zzO9aMo33jA3pPv8SoYskw  EhplcymNbSX5TvPgGilL7Q    5.0\n",
       "197093  zzO9aMo33jA3pPv8SoYskw  pDewiJY6KCcZgLxxgxg13Q    5.0\n",
       "197094  zzO9aMo33jA3pPv8SoYskw  ylxqmxh2gO1yCpQkIk6o3A    5.0\n",
       "197095  zzO9aMo33jA3pPv8SoYskw  5eK_pgro9_LxPYDoRVJnEA    2.0\n",
       "197096  zzO9aMo33jA3pPv8SoYskw  frCxZS7lPhEnQRJ3UY6m7A    5.0\n",
       "\n",
       "[197097 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('../data/train_df.pkl')\n",
    "valid_df.to_pickle('../data/valid_df.pkl')\n",
    "test_df.to_pickle('../data/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
